/* Author - Saugat Kandel 

-------------------------------------------------------------------------
Alternating Directions Implicit Method for the Linearized Euler Equations
-------------------------------------------------------------------------
- Refer to section 35.3 of the HPCbook for problem description.

- Notation used borrows from the matlab code provided by the professor.

- Uses LU factorization from LAPACK for the linear solving.
- Calculates the LU factorizations once, then reuses them. 
- Uses Matrix multiplication from CBLAS.

- I tried (but failed) to change the code to use column major storage, 
  which would make the MPI scatter and gather calls easier,
  and also make the LAPACK and CBLAS calls faster. 
- Sticking to row major storage. 

- Basic idea: divide up the columns of the V matrix among the threads. 
- Matrix multiplication and linear solving can be performed for individual cols.
- Need to gather all the columns for transpose ops.
*/


#include <stdio.h>
#include "cblas.h"
#include "lapacke.h"
#include "funcs.h"
#include "mpi.h"

int main(int argc, char* argv[]){
    
    MPI_Init(&argc, &argv);
    
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    // Start the timing
    double starttime = MPI_Wtime();
    
    // Some default parameters. 
    // Can override N, M, T and fname through the command line. 
    double gamma = 1.4;
    int M = 100;
    int N = 100;
    double T = 2;
    
    // Creating N points between x0 and x1
    double x0 = -1;
    double x1 = 1;
    
    // Output file name
    char *fname = "euler2d.out";
    
    if (argc > 1)
        N = (int)atof(argv[1]);
    if (argc > 2)
        M = (int)atol(argv[2]);
    if (argc > 3)
        T = (double)atol(argv[3]);
    if (argc > 4)
        fname = argv[4];    

    
#ifdef VERBOSE
    // Using a preprocessor flag to control verbose/debug output
    if (rank == 0){
        printf("Parameters are:\n");
        printf("apha: %f\n", gamma);
        printf("Number of time steps (M): %d\n", M);
        printf("Number of grid points in each dimension (N): %d\n", N);
        printf("Total terminal time (T): %f\n", T);
        printf("Output filename is %s\n", fname);
    }
#endif
    
    double dt = (double) T / M;
    
    // Matrix V is generated by vertically stacking the NxN matrices
    // {rho, u, v, p}
    double* V = (double*)malloc(sizeof(double) * 4 * N * N);
    
    // This is a convenience matrix that alternately stores the V values.
    double* V2 = (double*)malloc(sizeof(double) * 4 * N * N);
    
    // The operating matrices for the ADI method.
    double* MRx = (double*)malloc(sizeof(double) * 4 * 4 * N * N);
    double* MRy = malloc(sizeof(double) * 4 * 4 * N * N);
    
    // Don't need to store this outside of rank 0 process. 
    // Using the LU factorization for the actual linear solving.
    double* MLx = NULL;
    double* MLy = NULL;
    
    // Setting up for LU factorizaton.
    lapack_int n, lda, info_mly, info_mlx;
    n = 4 * N; lda = 4 * N;
    double* MLy_LU = (double*)malloc(sizeof(double) * 4 * 4 * N * N);
    double* MLx_LU = (double*)malloc(sizeof(double) * 4 * 4 * N * N);
    
    lapack_int* ipiv_mly = (lapack_int*)malloc(sizeof(lapack_int) * 4 * N);
    lapack_int* ipiv_mlx = (lapack_int*)malloc(sizeof(lapack_int) * 4 * N);
    
    
    // No point in recalculating these same values in every thread.
    if (rank==0){
        MLx = (double*) malloc(sizeof(double) * 4 * 4 * N * N);
        MLy = (double*) malloc(sizeof(double) * 4 * 4 * N * N);
        initMatrices(V, MRx, MRy, MLx, MLy, N, x0, x1, dt, gamma);
        
        // The LAPACK dgetrf call calculates the LU factorization for the 
        // input matrix, then stores them *in* the input matrix. 
        // Hence why we copy MLy and MLx.
        copy_2d_array(MLy, MLy_LU, 4 * N, 4 * N);
        copy_2d_array(MLx, MLx_LU, 4 * N, 4 * N);
        
        info_mly = LAPACKE_dgetrf(LAPACK_ROW_MAJOR, n, n, MLy_LU, lda, ipiv_mly);
        info_mlx = LAPACKE_dgetrf(LAPACK_ROW_MAJOR, n, n, MLx_LU, lda, ipiv_mlx);
        
#ifdef VERBOSE
        // info value should always be 0. Greater than or less than 0 indicates failure.
        printf("LU Factorization: info_mly %d  info_mlx %d\n", info_mly, info_mlx);
#endif
    }
    // Broadcasting the matrix
    // MPI_Bcast(V, 4 * N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(MRx, 4 * 4 * N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(MRy, 4 * 4 * N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(MLy_LU, 4 * 4 * N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(MLx_LU, 4 * 4 * N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(ipiv_mly, 4 * N, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(ipiv_mlx, 4 * N, MPI_INT, 0, MPI_COMM_WORLD);
    
    // making sure all the threads complete up to this step
    MPI_Barrier(MPI_COMM_WORLD);
    
    // calculating the number of columns (of V) to assign to each thread.
    // if N is a multiple of the num of threads, then localN = N / size
    // Otherwise, the lower ranked threads get the remainder jobs one by one
    int localN = ((N % size) > rank ? 1 : 0) + N / size;
#ifdef VERBOSE
    printf("size %d, rank %d, localN %d\n", size, rank, localN);
#endif
    
    // Storing (in rank 0 thread) the number of columns assigned to each thread
    int* all_localN = NULL;
    if (rank == 0){
        all_localN = (int*)malloc(sizeof(int) * size);
    }
    MPI_Gather(&localN, 1, MPI_INT, all_localN, 1, MPI_INT, 0, MPI_COMM_WORLD);
    
    // Since different threads can have different numbers of columns assigned, 
    // and hence different sizes of data to send/receive, we need to use
    // Gatherv, Scatterv.
    // Need to calculate 
    int* send_counts = NULL;
    int* displacements = NULL;
    if (rank == 0){
        send_counts = (int*)malloc(sizeof(int) * size);
        displacements = (int*)malloc(sizeof(int) * size);
        
        for (int i=0; i < size; i++){
            send_counts[i] = all_localN[i];
            displacements[i] = i > 0 ? displacements[i-1] + send_counts[i-1] : 0;
        }
    }
    
    // Since I am using row major storage for the V matrix, it is not intuitive
    // how exactly to divvy up the columns for the different threads. 
    // Using the method described here:
    // https://stackoverflow.com/questions/5371733/how-to-mpi-gatherv-columns-from-processor-where-each-process-may-send-different/5373104#5373104
    MPI_Datatype a_col_type, new_a_col_type;
    MPI_Type_vector(4 * N, 1, localN, MPI_DOUBLE, &a_col_type);
    MPI_Type_commit(&a_col_type);

    /* make the type have extent 1 double -- now the next
     * column starts in the next double of the array 
     */
    MPI_Type_create_resized(a_col_type, 0, 1*sizeof(double), &new_a_col_type);
    MPI_Type_commit(&new_a_col_type);

    MPI_Datatype b_col_type, new_b_col_type;
    MPI_Type_vector(4 * N, 1, N, MPI_DOUBLE, &b_col_type);
    MPI_Type_commit(&b_col_type);

    /* similarly "resize" b columns */
    MPI_Type_create_resized(b_col_type, 0, 1*sizeof(double), &new_b_col_type);
    MPI_Type_commit(&new_b_col_type);
    
    // Prepping for the per-thread operations.
    lapack_int local_nrhs, local_ldb;
    local_nrhs = localN; local_ldb = localN;
    double* V_local = (double*)malloc(sizeof(double) * 4 * N * localN);
    double* V_local_2 = (double*)malloc(sizeof(double) * 4 * N * localN);
    
    // Scattering the columns
    MPI_Scatterv(V, send_counts, displacements, new_b_col_type,
                 V_local, localN, new_a_col_type, 0, MPI_COMM_WORLD);
 
    for (int t=0; t<M; t++){
        
        // matrix multiplication can be done per-thread
        cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, 
               4 * N, localN, 4 * N, 1, MRx, 4 * N, V_local, localN, 0, V_local_2, localN);
        
        // gather to rank 0 for blockwise transpose
        MPI_Gatherv(V_local_2, localN, new_a_col_type, 
                    V2, send_counts, displacements, new_b_col_type,
                    0, MPI_COMM_WORLD);
        if(rank == 0)
            transpose_blockwise(V2, V, N);
        
        // scatter for linear solving and matrix multiplication
        MPI_Scatterv(V, send_counts, displacements, new_b_col_type,
                     V_local, localN, new_a_col_type, 0, MPI_COMM_WORLD);
        
        info_mly = LAPACKE_dgetrs(LAPACK_ROW_MAJOR, 'N', n, local_nrhs, MLy_LU, 
                                  lda, ipiv_mly, V_local, local_ldb);
        
        cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, 
                    4 * N, localN, 4 * N, 1, MRy, 4 * N, V_local, localN, 0, V_local_2, localN);
        
        // gather for transpose
        MPI_Gatherv(V_local_2, localN, new_a_col_type, 
                    V2, send_counts, displacements, new_b_col_type,
                    0, MPI_COMM_WORLD);
        if(rank == 0)
            transpose_blockwise(V2, V, N);
        
        // scatter again
        MPI_Scatterv(V, send_counts, displacements, new_b_col_type,
                     V_local, localN, new_a_col_type, 0, MPI_COMM_WORLD);
        
        info_mlx = LAPACKE_dgetrs(LAPACK_ROW_MAJOR, 'N', n, local_nrhs, MLx_LU,
                                  lda, ipiv_mlx, V_local, local_ldb);
#ifdef VERBOSE
        // info values should be 0 always. greater than or less than 0 indicates some error.
        printf("info_mly %d  info_mlx %d\n", info_mly, info_mlx);
#endif
    }  
    
    // finally gathering to store in a file.
    MPI_Gatherv(V_local, localN, new_a_col_type, 
                V, send_counts, displacements, new_b_col_type,
                0, MPI_COMM_WORLD);
    
    if (rank == 0){
        printf("\n\nruntime %g\n\n", (MPI_Wtime() - starttime));
    
        FILE* output = fopen(fname, "w");
        fwrite(V, sizeof(double), 4 * N * N, output);
        fclose(output);
    }
    
    free(V2);                             
    free(V);
    free(V_local);
    free(V_local_2);
    free(send_counts);
    free(displacements);
    free(MLx);
    free(MRx);
    free(MLy);
    free(MRy);
    free(MLy_LU);
    free(MLx_LU);
    free(ipiv_mly);
    free(ipiv_mlx);
    
    MPI_Type_free(&a_col_type);
    MPI_Type_free(&new_a_col_type);
    MPI_Type_free(&b_col_type);
    MPI_Type_free(&new_b_col_type);
    MPI_Finalize();
    return 0;
}
